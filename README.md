# Классификация постов ВКонтакте по тематике железнодорожного и водного транспорта

Этот репозиторий содержит код и ноутбук для курсовой работы по дисциплине  
**«Анализ больших текстовых данных и текстовый поиск»**.

Цель проекта — по тексту поста из социальной сети **ВКонтакте** определить,  
был ли он опубликован в сообществе про **железнодорожный транспорт** или про **водный транспорт**.

---

## 1. Описание задачи

Во ВКонтакте существует множество транспортных сообществ: про железные дороги, водный транспорт, авиацию, городской транспорт и т.д.  
Каждое сообщество формирует свой характерный «лексический профиль»: устойчивые слова и выражения, специфические для тематики.

В этом проекте:

- выбираются два конкретных сообщества: одно про железные дороги, другое про водный транспорт;
- с помощью VK API собираются тексты постов со стен этих групп;
- тексты проходят предобработку: очистка, лемматизация, удаление стоп-слов;
- строится векторное представление документов методом **TF-IDF** (униграммы и биграммы);
- обучаются и сравниваются две модели:
  - **Multinomial Naive Bayes**  
  - **Logistic Regression**
- оценивается качество (accuracy, F1) и интерпретируются наиболее информативные слова/н-граммы для каждой тематики.

---

## 2. Стек технологий

Проект реализован в Jupyter Notebook на языке **Python 3**.  
Используемые библиотеки:

- Работа с данными: `pandas`, `numpy`
- Визуализация: `matplotlib`, `seaborn`
- VK API: `requests`, `vk_api`
- Обработка текста: `nltk`, `razdel`, `pymorphy2`, `emoji`
- Модели и метрики: `scikit-learn`

Рекомендуемый набор пакетов (`requirements.txt`):

```txt
pandas
numpy
matplotlib
seaborn
requests
vk-api
nltk
razdel
pymorphy2
emoji
scikit-learn
````

Установка:

```bash
pip install -r requirements.txt
# или
pip install pandas numpy matplotlib seaborn requests vk-api nltk razdel pymorphy2 emoji scikit-learn
```

---

## 3. Структура репозитория

```text
.
├── ВохминАЕ_Курсовая_АнализТекстов.ipynb   # основной Jupyter-ноутбук с кодом
├── README.md                               # описание проекта (этот файл)
└── .gitignore                              # настройки игнорируемых файлов
```

Файл с токеном VK (`vk_token.txt`) **не должен** попадать в репозиторий.
Его нужно создать локально и добавить в `.gitignore`.

Пример `.gitignore`:

```gitignore
vk_token.txt
.ipynb_checkpoints/
__pycache__/
*.pyc
```

---

## 4. Настройка VK API

1. Создать standalone-приложение во ВКонтакте (через [https://dev.vk.com](https://dev.vk.com)).
2. Получить `access_token` с правами чтения стен сообществ.
3. В корне проекта создать файл `vk_token.txt` и записать туда токен **одной строкой**.
4. Убедиться, что `vk_token.txt` добавлен в `.gitignore`.

В ноутбуке токен подхватывается автоматически:

```python
TOKEN_FILE = "vk_token.txt"

with open(TOKEN_FILE, "r") as f:
    ACCESS_TOKEN = f.read().strip()
```

---

## 5. Как запустить ноутбук

1. Клонировать репозиторий или скачать архив:

   ```bash
   git clone https://github.com/your-username/your-repo-name.git
   cd your-repo-name
   ```

2. Установить зависимости (см. раздел выше).

3. Положить рядом с ноутбуком файл `vk_token.txt` с вашим токеном VK.

4. Запустить Jupyter:

   ```bash
   jupyter notebook
   ```

   и открыть файл
   **`ВохминАЕ_Курсовая_АнализТекстов.ipynb`**.

5. Выполнить ячейки сверху вниз:

   * импорт библиотек и настройка окружения;
   * чтение токена и задание ID двух групп VK (железные дороги / водный транспорт);
   * сбор постов методом `wall.get` в два датафрейма;
   * формирование единого корпуса, удаление пустых, коротких текстов и дубликатов;
   * предобработка (`clean_text`, `normalize_text`), формирование `text_clean` и `text_norm`;
   * разведочный анализ (EDA): баланс классов, длина текстов, топ-леммы;
   * разбиение на train/test, TF-IDF-векторизация;
   * обучение моделей (Naive Bayes, Logistic Regression), расчёт метрик и матриц ошибок;
   * анализ весов логистической регрессии и функция `classify(text)`.

---

## 6. Логика пайплайна

1. **Сбор данных**
   Выгрузка постов из двух сообществ ВКонтакте с помощью VK API (`wall.get`)
   с сохранением текста и базовых метаданных.

2. **Формирование корпуса**
   Объединение постов в общий `DataFrame`, добавление меток классов (`rail` / `water`),
   фильтрация пустых и слишком коротких сообщений, удаление дубликатов.

3. **Предобработка текста**
   Очистка (`clean_text`): ссылки, упоминания, HTML-сущности, эмодзи, лишние символы.
   Нормализация (`normalize_text`): токенизация, лемматизация, удаление стоп-слов и коротких токенов.
   Результат — колонка `text_norm`.

4. **Разведочный анализ (EDA)**
   Анализ длины сообщений и баланса классов,
   построение топ-лемм для каждой группы.

5. **Векторизация и обучение моделей**
   Построение TF-IDF-признаков (униграммы + биграммы).
   Обучение `MultinomialNB` и `LogisticRegression` на обучающей выборке.
   Оценка на тесте: accuracy, F1, матрицы ошибок.

6. **Интерпретация и демонстрация**
   Анализ коэффициентов логистической регрессии: слова и н-граммы, характерные для каждой тематики.
   Функция `classify(text)` для интерактивной проверки работы модели.

---

## 7. Результаты

* Обе модели (Naive Bayes и Logistic Regression) на тестовой выборке показывают
  **почти идеальное качество** (accuracy и F1 близки к 1.0).
* Матрицы ошибок не содержат неверных классификаций: выбранные сообщества хорошо различимы по лексике.
* По весам логистической регрессии видно, что:

  * для железнодорожной тематики чаще всего встречаются слова, связанные с поездами, станциями, путями, расписанием;
  * для водного транспорта — слова, описывающие суда, рейсы, порты, причалы, навигацию.

---

## 8. Возможное развитие

Возможные направления развития проекта:

* добавить новые классы (авиация, городской транспорт и др.) и перейти к многоклассовой классификации;
* сравнить TF-IDF с более сложными представлениями текста (word2vec, fastText, BERT-модели);
* завернуть модель в простой веб-сервис или Telegram-бота, который по введённому тексту
  определяет, к какой транспортной тематике он ближе.

---

## 9. Автор

Курсовая работа по дисциплине
**«Анализ больших текстовых данных и текстовый поиск» (РУТ (МИИТ))**.

Ноутбук: `ВохминАЕ_Курсовая_АнализТекстов.ipynb`.

```
```
